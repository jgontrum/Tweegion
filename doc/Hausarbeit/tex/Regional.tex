\documentclass[../Main.tex]{subfiles} 
\begin{document}

\section{Regional word attempt (8p)}
\subsection{Idea}
Die Grundidee des \textit{Regiowort-Ansatzes} basiert auf Ausdrücken der regionalen Alltagssprache. Hierunter fallen Ausdrücke, die innerhalb des deutschen Sprachraums einigermaßen komplementär verteilt sind und die von Sprechern in normaler Umgangssprache, also nicht unbedingt nur mundartlich, verwendet werden. Prominente Beispiele sind \textit{Samstag} versus \textit{Sonnabend} und \textit{viertel vor} versus \textit{dreiviertel}. Da wir uns der Einfachheit halber auf Unigramme beschränkt haben, fällt letzteres leider aus unserem Untersuchungsbereich heraus.

\subsection{Initial data}
Als Quelle für die regionalen Ausdrücke verwendeten wir den Atlas der Alltagssprache. Der Atlas ist ein gemeinsames Projekt der Universität Salzburg und der Université de Liège und basiert auf Umfragen unter Sprechern im gesamten deutschen Sprachraum. Die Ergebnisse wurden in Kartenform mit Erläuterungen veröffentlicht.

Beim Sammeln der Daten aus dem Atlas der Alltagssprache mussten verschiedene Faktoren berücksichtigt werden. In Frage kamen natürlich nur Eigenheiten, die sich im Schriftbild niederschlagen; Daten etwa zu Vokalqualitäten fielen somit heraus. Zuvörderst waren wir auf Ausdrücke aus, die den Sprachraum möglichst klar aufteilen, also wenig Überlappung zeigen. Dabei sollten sie auch tatsächlich regional signifikant sein, also nicht etwa wie \textit{Backofen} nahezu den gesamten Sprachraum abdecken. Unsere Obergrenze war hier die Zuordnung zu vier unserer sieben Regionen. Des Weiteren waren Homonyme für unsere Zwecke ungeeignet, sodass etwa die meisten regionalen Wörter für \textit{Dachboden,} darunter \textit{Boden}, \textit{Speicher} und \textit{Bühne}, ausschieden.

Aus den Daten erstellten wir die initialen Wortvektoren dieses Ansatzes in Form einer CSV-Datei (comma separated values). Wir ordneten dabei jedes Wort $n$ unserer sieben Regionen zu. Für diese Regionen erhielt der Wortvektor den Wert $\frac 1 n$, für alle anderen den Wert 0.

\subsection{Data accumulation}
Auf Grundlage der Quelle erstellten wir etwa 200 initiale Wortvektoren. Mit einer solch geringen Menge von Daten ist natürlich noch keine Klassifikation von Tweets möglich -- es gäbe im Text des Tweets schlicht nur mit geringster Wahrscheinlichkeit überhaupt einen Treffer für eines dieser 200 Wörter.

Daher kamen wir auf die Idee einer Anreicherungsphase mithilfe von Twitter-Trainingsdaten. Die Idee dabei ist, nach Tweets zu suchen, die unsere initialen Wörter enthalten, und alle weiteren darin vorkommenden Wörter ebenfalls als regionale Wörter zu werten.

Um dabei keine groben Fehler zu begehen, entfernten wir aus dem Trainingskorpus neben Usertags, Hashtags und URLs auch Stoppwörter. Außerdem werden diese sekundären Wortvektoren basierend auf allen gefundenen Tweets berechnet, in den das jeweilige Wort auftritt. So sollten nicht-regionale Wörter, die natürlich auch in den gefundenen Tweets vorkommen, letztlich einen durchschnittlichen, regional insignifikanten Vektor erhalten.

Um tatsächlich genügend Daten zu erhalten, überlegten wir uns, diese Anreicherung mit den neu gefundenen Wortvektoren zu wiederholen. So entstand unser Haupt-Algorithmus, der in einer Schleife beliebig oft ausgeführt werden kann, siehe den Abschnitt zu Algorithmen.

\subsubsection{Guessing the amount of regional Tweets}
\begin{figure}
\begin{tikzpicture}
       \pgfplotstableread{../data/geo_cos.csv}\data
       \begin{axis}[
           legend pos=south east,
           ylabel near ticks,
           axis y line*=left,
           xmin = 5,
           xmax = 100,
           ylabel= accuracy,
           xlabel= guessed percentage of non-regional Tweets,
           xtick = {5,10,20,30,40,50,60,70,80,90,100},
           width=0.9\columnwidth,
           height=0.5\textheight]
           \addplot[blue, mark=x] 
           table[x=Guess,y=Accuracy]{\data} ;
           \addlegendentry[font=\tiny]{accuracy}
       \end{axis}
       \begin{axis}[
           hide x axis,
           xmin = 5,
           xmax = 100,
           axis y line*=right,
           legend pos=south west,
           ylabel near ticks,
           ylabel= cosine similarity threshold,
           width=0.9\columnwidth,
           height=0.5\textheight]
           \addplot[red, mark=x] 
           table[x=Guess,y=Threshold]{\data};
           \addlegendentry[font=\tiny]{threshold}
       \end{axis}
\end{tikzpicture}
Calculation method: \textit{Normalized}; Geo-dataset: \textit{balanced-39k}, Main-dataset: \textit{geo-175k}; Stopwords: \textit{Top 200}; Loops: \textit{1}; 

  \caption{Relation between the amount of regional Tweets and the accuracy.}
  \label{geo_graph2}
\end{figure}

\subsection{Experiments}
\subsubsection{Parameters}
\subsubsection{Expectations}
\subsubsection{Discussion}
\subsubsection{-> new Experiment}
\subsection{Conclusion}

\end{document}
