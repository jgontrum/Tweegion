\documentclass[../Main.tex]{subfiles} 
\begin{document}

\section{Regional word attempt (8p)}
\subsection{Idea}
The main idea behind the regional word attempt is based on expressions of regional everyday language. This comprises expressions which are distributed somewhat complementary within the German-speaking area and are used by speakers in normal colloquial language, i. e. not necessarily just dialectal. Famous examples include \textit{Samstag} versus \textit{Sonnabend} and \textit{viertel vor} versus \textit{dreiviertel}. Since we restricted ourselves to unigrams for the sake of simplicity, the latter falls out of our examination's range.

\subsection{Initial data}
As a source for the regional expressions we used the \textit{Atlas der Alltagssprache.} The Atlas is a collaborative projet of the Universität Salzburg and the Université de Liège and is based on surveys of speakers from anywhere within the German-speaking area. The results were published in the form of maps along with explanations.

Collecting data from the Atlas der Alltagssprache we had to consider several factors. Of course only idiosyncrasies which are reflected in the written form came into consideration; data regarding for instance vowel qualities were out of the question. First of all we were looking for expressions which would divide the language area precisely as possible, i. e. show little to no overlapping. At the same time they should actually be regionally salient and not cover almost the whole language area like for instance \textit{Backofen.} Our limit here was to assign a word to four of our seven regions. Furthermore, homonyms and polysemes were inappropriate for our purposes, so for example most of the regional words for \textit{attic,} including \textit{Boden,} \textit{Speicher} and \textit{Bühne,} were ruled out. We also went without very short expressions like \textit{wa} (Berlin dialect for \textit{...right?}) because of the high chance of coincidence with abbreviations, cutted forms etc.

From the data we created the initial word vectors for this attempt in the form of a csv file \textit{(comma separated values).} Doing this we assigned every word to $n$ of our seven regions. For this regions the word vector got the value $\frac 1 n$, for the remaining ones it got the value 0.

\subsection{Data accumulation}
On the basis of the source we created about 200 initial word vectors. Of course no classification of Tweets is possible using such a small amount of data---the chance of even finding a single one of these 200 words in a Tweet's text is simply to low.

That's why we came up with the idea of an enrichment stage with the aid of training data from Twitter. The idea is to search for Tweets containing our initial words and to grade all further words occuring in their text as regional words, too.

To avoid gross errors doing this, we removed stopwords beside usertags, hashtags and URLs from the training corpus. What's more, these derivative word vectors are calculated based on all found Tweets containing the respective word. So non-regional words, which naturally also occur in those Tweets, eventually get an average-like, regionally indifferent vector.

In order to actually gain enough data we thought of repeating this accumulation using the newly acquired word vectors. That's how our main algorithm came into being, which can be executed in a loop with any number of cycles, see chapter \textit{Algorithms.}

\subsubsection{Guessing the amount of regional Tweets}
\begin{figure}
\begin{tikzpicture}
       \pgfplotstableread{../data/geo_cos.csv}\data
       \begin{axis}[
           legend pos=south east,
           ylabel near ticks,
           axis y line*=left,
           xmin = 5,
           xmax = 100,
           ylabel= accuracy,
           xlabel= guessed percentage of non-regional Tweets,
           xtick = {5,10,20,30,40,50,60,70,80,90,100},
           width=0.9\columnwidth,
           height=0.5\textheight]
           \addplot[blue, mark=x] 
           table[x=Guess,y=Accuracy]{\data} ;
           \addlegendentry[font=\tiny]{accuracy}
       \end{axis}
       \begin{axis}[
           hide x axis,
           xmin = 5,
           xmax = 100,
           axis y line*=right,
           legend pos=south west,
           ylabel near ticks,
           ylabel= cosine similarity threshold,
           width=0.9\columnwidth,
           height=0.5\textheight]
           \addplot[red, mark=x] 
           table[x=Guess,y=Threshold]{\data};
           \addlegendentry[font=\tiny]{threshold}
       \end{axis}
\end{tikzpicture}
Calculation method: \textit{Normalized}; Geo-dataset: \textit{balanced-39k}, Main-dataset: \textit{geo-175k}; Stopwords: \textit{Top 200}; Loops: \textit{1}; 

  \caption{Relation between the amount of regional Tweets and the accuracy.}
  \label{geo_graph2}
\end{figure}

\subsection{Experiments}
\subsubsection{Parameters}
\subsubsection{Expectations}
\subsubsection{Discussion}
\subsubsection{-> new Experiment}
\subsection{Conclusion}

\end{document}
