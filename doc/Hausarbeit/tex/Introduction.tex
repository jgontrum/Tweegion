\documentclass[./Main.tex]{subfiles} 
\begin{document}
\section{Introduction}
\subsection{Twitter}
Twitter is a web service for sending out short messages---so called microblogging---originated in 2006 and used worldwide nowadays. Only a small fraction of the messages, the so-called Tweets, is written in German. Predominantly used languages are English, Spanish, Portuguese, Japanese, and Malay.\cite{TwLang} Also when compared to the population, Germans, Austrians and Swiss do not Tweet very much, whereas the network is extremely popular for instance in the Netherlands, the US, the UK, and Japan.\cite{TwCountries}

The spatial distribution of sended Tweets within Germany is investigable based on geo annotation, too. It mainly reflects the distribution of population; focal points are above all Berlin and the Ruhr district. But---beside Tweeting in general---especially sending of geo data (so-called geotagging) is rather unpopular in the German-speaking area.

There are three fundamentally different location-related pieces of information that can be sended along with a Tweet. None of these specifications is obligatory for the user, so local dara are not available for all Tweets.

First of all, the user may enter a location to their profile. In a JSON object, it appears as field \textit{location} in the sub-object \textit{user.} Users often enter fantasy places like \textit{Wolkenkuckucksheim (cloud cuckoo land)} here, though. What's more, the information may be ambigous and inaccurate, which is why they are practically ruled out for computational processing.

When sending a Tweet, there is also the option \textit{add location.} There a distinct, named location can be chosen. In a JSON object, it appears as sub-object \textit{place,} including name, type (e. g. \textit{city}), country and a \textit{bounding box,} a rectangle enclosing the place defined by four geo coordinates. As the Twitter developers documentation puts it, ``Tweets associated with places are not necessarily issued from that location but could also potentially be \textit{about} that location''. 

Finally it is possible to send a Tweet's actual point of origin from GPS-enabled devices. The coordinates are saved as sub-object \textit{geo} in the JSON object.

\subsection{General Idea}
Our aim is a rough placement of Tweets within the German-speaking area in spite of the above-mentioned rare geo data and barely usable location information. As computational linguists we came up with the idea to use language as an indicator of origin---the pure text of a Tweet already contains information about its origin.

There are two different items here. One the one hand there is the place or the places where a Twitter user has grown up and which influenced his or her language. Mainly those places is what we measure when we search for regional salient expressions of dialectal and colloquial speech. One the other hand there is the current, sometimes very short-timed whereabout which the user is Tweeting from. On a textual level it will rather be reflected in location-related terms (place names, traffic hubs, local events and personages etc.). Furthermore it is what we get to know from Tweets' geo data. Yet geo data are the only data we can use for evaluation of our results. While our geo data driven \emph{Geo-location-attempt} may be evaluated quite suitably, our \emph{Regional-words-attempt} does specifically aim for the item `language-imprinting origin of a Twitter user' and will therefore inevitably pass the evaluation with a certain handicap.

Our attempt is to divide the German-speaking area into certain regions and to use machine learning to collect regionally salient data from the particular regions. We use the simple bag-of-words model, i. e. we consider unigrams.



\subsection{Geo-location-attempt}
As allready mentioned above, German Twitter-users only seldom allow Twitter to store geo data. But nonetheless there is a small number of Tweet containing those geo data. The \emph{Geo-location-attempt} uses those data by assigning a specific region to each Tweet. Based on that information we calculate affiliation of all words of the Tweet with particular regions. This will be done for all Tweets in the training corpus and so eventually we create a vector-space model based on the exact knowledge of geographical origins of all Tweet writers.

\subsection{Regional-words-attempt}
The \emph{Regional-words-attempt} was inspired by the fact that even for such ordinary things like small bakery products or other food exists a confusing mess of words. There is the funny effect that some of these foods got names of towns different from the speakers-town. So a small sausage in Berlin is called \textit{Wiener} whilst in Vienna it is called \textit{Frankfurter.} This shows that the use of language and especially the use of lexical items differs throughout regions. The aim of our regional word attempt was to exploit this fact, try to map different words to specific regions, and to create a vector-space model of words for each region. In doing so we should be able to compare new Tweets against these models and assign it to one region.

\subsection{Expectations}
We expected that it was possible to map Tweets to a specific region if this Tweet contains material which will be relatively prominent in this region. But we were aware about the fact that a list serving as the base of our \emph{Regional-words-attempt} was very vulnerable to criticism because we ourselves lay the ground for further calculations. This means that not the real use of language of Twitter-users were the starting point of this attempt but academical investigations. The opposite is the case for the Geo-location-attempt: Here all calculations were done based on real Twitter data.

We therefore expected our \emph{Geo-location-attempt} to show better results than the \emph{Regional-words-attempt}.

\subsection{Sources, used corpora}
We created two main-corpora: \emph{Regional word corpus} and \emph{Geo data corpus}.

For our regional word corpus we created a list of words which we thought would be good candidates to represent one or at maximum four regions. This list of words was inspired by the work of the \emph{Atlas der deutschen Alltagssprache} -- a project of Universität Salzburg and Université de Liège which aim is to collect information about everday use of different words of native German speakers depending on their origin. This list was used to collect 12,878 Tweets from August 1, 2013 to August 8, 2013 using the Twitter-API (Regioword-corpus). Because we thought that some regio-words will be unknown or very rare we decided to switch off language detection by \emph{LangID} and to check only if a Tweet containing one of our words got one of the country IDs \textit{de, at} or \textit{ch.} We are aware of the fact that by chance some of our regio-words could also be words in other languages but we thought that combining this word with the country IDs was sufficient to minimize that chance. For reasons of comparison additionally we created two corpora whose content probably don't contain words from out list - at least we don't care about it: a corpus out of 12,878 Tweets (Scheffler-Regioword-corpus) and a corpus containing all 1,494.994 Tweets (Scheffler-oneday-corpus) from August 29, 2012 - both from the Scheffler-corpus (see below). \\
Our Geo data corpus has been extracted from the Scheffler-corpus.
We created balanced sets, where the amount of Tweets are the same for all regions. Because the fewest number of Tweets (8787) came from region 6, \textit{Österreich,} we ended up with only 61509 Tweets for all regions.

The source for parts of the regional word corpus and geo data corpus was the \emph{Scheffler-corpus}. This is a collection containing Tweets from April 1, 2012 to April 30, 2012 collected by Tatjana Scheffler using the Twitter-API which are recognized as German Tweets by \emph{LangID.}

\subsection{Regions}
The considerations of our specific division of the German-speaking area into regions is based on were mainly predetermined by the \emph{Regional word attempt}. The data situation in the \emph{Atlas der deutschen Alltagssprache} showed us the scale at which we could divide the language area based on regional everyday language. We also did the actual defining of the particular regions examining the data there. But Twitter as our field of application did contribute a crucial criterion here, too: We wanted regions to be formed that would provide a sufficient amount of Tweets sent from there. So for instance the data of the \emph{Atlas der deutschen Alltagssprache} also showed a small region around Saarland and Luxembourg to have characteristic idiosyncrasies, but we did not carry over it because of too few expectable Tweets.
\end{document}
