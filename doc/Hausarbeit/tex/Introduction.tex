\documentclass[../Main.tex]{subfiles} 
\begin{document}
\begin{abstract}
This project originated from the seminar \textit{Computerlinguistische Analyse von Twitterdaten} by Tatjana Scheffler at Universität Potsdam in summer 2013. During this seminar we were confronted with many problems of computational analysis of social media data, esp. very short texts of media like Twitter.

This project's aim was to find a way to map unseen Tweets to the region the Twitter user comes from. We were aware it would only be possible to map a tweet to a specific region if this tweet contained at least little hints towards a region. Tweets written in pure standard language are impossible to map. Nonetheless we thought it being worthwhile to touch this region-specific marked tweets.

We tried out two different approaches whose starting points were different but which were very similar afterwards. The \emph{Regional words attempt} trained a model according to a list of words known of being regionally salient ones. The \emph{Geo location attempt} trained a model based on tweets which were sent together with geo data information. We found out that the regional words attempt was hardly usable whilst the geo location attempt could reach much better results.

But not only finding and implementing a good algorithm for fulfilling this aim was important. For the three co-workers of this project it was the first project of this manner and it was exciting to learn to develop source code in a team. So we had to learn to work together, think together, try to find a way to communicate and of course to develop code.

Even if the results of our project were not that satisfying as we wished they were, we nevertheless learnt much about collaborating in a project like this. Additionally we are convinced that bad results are improvable by finding more and better region-specific words and of course by accumulating more knowledge in computational linguistics during our studies. So the journey still goes on.
\end{abstract}

\section{Introduction (5p)}
\subsection{Twitter}
Twitter is a web service for sending out short messages---so called microblogging---originated in 2006 and used worldwide nowadays. Only a small fraction of the messages, the so-called Tweets, is written in German. Predominantly used languages are English, Spanish, Portuguese, Japanese, and Malay.\cite{TwLang} Also when compared to the population, Germans, Austrians and Swiss do not Tweet very much, whereas the network is extremely popular for instance in the Netherlands, the US, the UK, and Japan.\cite{TwCountries}

The spatial distribution of sended Tweets within Germany is investigable based on geo annotation, too. It mainly reflects the distribution of population; focal points are above all Berlin and the Ruhr district. But---beside Tweeting in general---especially sending of geo data (so-called geotagging) is rather unpopular in the German-speaking area.

There are three fundamentally different location-related pieces of information that can be sended along with a Tweet. None of these specifications is obligatory for the user, so local dara are not available for all Tweets.

First of all, the user may enter a location to their profile. In the JSON object, it appears as field \textit{location} in the sub-object \textit{user.} In our corpus this information is available for x\% of Tweets. Users often enter fantasy places like \textit{Wolkenkuckucksheim (cloud cuckoo land)} here, though. What's more, the information may be ambigous and inaccurate, which is why they are practically ruled out for computational processing.

When sending a Tweet, there is also the option \textit{add location.} There a distinct, named location can be chosen. In the JSON object, it appears as sub-object \textit{place,} including name, type (e. g. \textit{city}), country and a \textit{bounding box,} a rectangle enclosing the place defined by four geo coordinates. As the Twitter developers documentation puts it, ``Tweets associated with places are not necessarily issued from that location but could also potentially be \textit{about} that location.'' This information is available for x\% of Tweets in our corpus.

Finally it is possible to send a Tweet's actual point of origin from GPS-enabled devices. The coordinates are saved as sub-object \textit{geo} in the JSON object. x\% of the Tweets in our corpus supply this information.

\subsection{General Idea}
Our aim is a rough placement of Tweets within the German-speaking area in spite of the above-mentioned rare geo data and barely usable location information. We as computational linguists came up with the idea to use language as an indicator of origin---the pure text of a Tweet already contains information about its origin.

There are two different items here. One the one hand there is the place or the places where a Twitter user has grown up and which influenced his or her language. Mainly those places is what we measure when we search for regional salient expressions of dialectal and colloquial speech. One the other hand there is the current, sometimes very short-timed whereabout which the user is Tweeting from. On a textual level it will rather be reflected in location-related terms (place names, traffic hubs, local events and personages etc.). Furthermore it is what we get to know from Tweets' geo data. Yet geo data are the only data we can use for evaluation of our results. While our geo data driven attempt may be evaluated quite suitably, our regional word attempt does specifically aim for the item `language-imprinting origin of a Twitter user' and will therefore inevitably pass the evaluation with a certain handicap.

Our approach is to divide the German-speaking area into certain regions and to use machine learning to collect regionally salient data from the particular regions. We use the simple bag-of-words model, i. e. we consider unigrams.

\subsection{Regional words attempt}
This \emph{Regional words attempt} was inspired by the fact that even for such ordinary things like small bakery products or other food a confusing mess of words exists. With some of these prodcuts there is the funny effect that they got names of towns different from the speakers-town. So a small sausage in Berlin is called \textit{Wiener} whilst in Vienna it is called \textit{Frankfurter.} This shows that the use of language and especially the use of lexical items differs throughout regions. The aim of our regional word approach was to exploit this fact, try to map different words to specific regions, and to create a vector-space model of words for each region. In doing so we should be able to compare new Tweets against these models and assign it to one region.

\subsection{Geo location attempt}
As allready mentioned above, German Twitter users only seldom allow Twitter to store geo data. But nonetheless there is a small number of Tweet having those geo data. The \emph{Geo location attempt} uses those data by exactly assigning a specific region to each Tweet and based on that calculating the Tweet's words' affiliation with the particular regions. This will be done for all Tweets in the train corpus and so eventually we create a vector-space model again---this time based on the exact knowledge of geographical origins of all Tweet writers.

\subsection{Expectations}
We expected that it was possible to map tweets to a specific region if this Tweet contains material which will be relatively prominent in this region. But we were aware about the fact that the list serving as the base of our regional words attempt was very vulnerable to criticism because we ourselves lay the ground for further calculations. This means that not the real use of language of Twitter users were the starting point of this approach but academical investigations. The opposite is the case for the geo location attempt: Here all calculations were done based on real Twitter data.

We therefore expected our geo location attempt to show better results than the regional words attempt.

\subsection{Sources, used corpora}
We created two corpora: \emph{Regional word corpus} and \emph{Geo data corpus}.

For our regional word corpus we created a list of words which we thought would be good candidates to represent one or at maximum four regions. This list of words was inspired by the work of the \emph{Atlas der deutschen Alltagssprache}---a project of Universität Salzburg and Université de Liège which aim is to collect information about everday use of different words of native German speakers depending on their origin. This list was used to collect 12,878 tweets from August 1, 2013 to August 8, 2013 using the Twitter-API. Because we thought that some words will be unknown or very rare we decided to switch off language detection by \emph{LangID} and to only check if a tweet containing one of our words got on of the country IDs \textit{de, at} or \textit{ch.} We are aware of the fact that by chance some of our words could also be words in other languages but we thought that combining this word with the country IDs was sufficient to minimize that chance. In addition we created a corpus out of 12,878 tweets from the Scheffler corpus (see below) for reasons of comparison.

Our Geo data corpus has been extracted from the Scheffler corpus.
We created balanced sets, where the amount of Tweets are the same for all regions. Because the fewest number of Tweets (8787) came from region 6, \textit{Österreich,} we ended up with only 61509 Tweets for all regions.

The source for parts of the regional word corpus and geo data corpus was the \emph{Scheffler corpus}. This is a collection containing Tweets from April 1, 2012 to April 30, 2012 collected by Tatjana Scheffler using the Twitter-API which are recognized as German Tweets by \emph{LangID.}

\subsection{Regions}
The considerations our specific division of the German-speaking area into regions is based on were mainly predetermined by the regional word attempt. The data situation in the Atlas der deutschen Alltagssprache showed us the scale at which we could divide the language area based on regional everyday language. We also did the actual defining of the particular regions examining the data there. But Twitter as our field of application did contribute a crucial criterion here, too: We wanted regions to be formed that would provide a sufficient amount of Tweets sent from there. So for instance the data of the Atlas der deutschen Alltagssprache also showed a small region around Saarland and Luxembourg to have characteristic idiosyncrasies, but we did not carry over it because of too few expectable Tweets.

\end{document}
