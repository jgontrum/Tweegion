\documentclass[../Main.tex]{subfiles} 
\begin{document}
\begin{abstract}
This project originated from the seminar "Computerlinguistische Analyse von Twitterdaten" by Tatjana Scheffler at University Potsdam in summer 2013. During this seminar we were confronted with many problems of computational analysis of social media data esp. very short texts of media like Twitter.
This projects aim was to find a way to map some tweets of unknown twitter-users to the region the tweet or at least the user was sent or comes from. We were aware that it was only possible to map a tweet to a specific region if this tweet contains at least little hints to a region. Tweets written in high german are impossible to map. Nonetheless we thought it beeing worthwhile to touch this region-specific marked tweets.
We tried out two different approaches whose starting points were different but were very similar afterwards. The \emph{Regio-words-approach} trained a model according to a list of words known of beeing very regional ones. The \emph{Geo-location-approach} trained a model based on tweets which were sent together with geo data information. We find out that the \emph{Regio-words-approach} was hardly usable whilst the \emph{Geo-location-approach} could reach much better results. 
But not only finding and implementing a good algorithm for fulfilling this aim was important. For the three co-workers of this project it was the first project of this manner and it was exciting to learn to develop source code in a team - or at least: try to do so. So we had to learn to work together, think together, try to find a way to communicate and of course to develop code.
Even if the results of our project were not that satisfying as we wished they were we nevertheless learnt much about collaborating in a project like this. Additionally we are convinced that bad results are improvable by finding more and better region-specific words and of course by accumulating more knowledge in computational linguistics during our studies. So the journey still goes on.
\end{abstract}

\section{Introduction (5p)}
\subsection{Twitter} % State of the Art und so
Twitter ist ein seit 2006 bestehender und heute weltweit genutzter Webdienst zur Versendung von Kurznachrichten (sogenanntes Mikroblogging). Nur ein kleiner Teil der Nachrichten, der sogenannten Tweets, ist in deutscher Sprache verfasst. Die überwiegend genutzten Sprachen sind Englisch, Spanisch, (...). Auch im relativen Vergleich zur Einwohnerzahl zeigt sich, dass in Deutschland, Österreich und der Schweiz wenig getwittert wird, während das Netzwerk etwa in den Niederlanden, Großbritannien, Japan und Indonesien extrem populär ist.

Auch innerhalb Deutschlands ist die räumliche Verteilung versendeter Tweets auf Grundlage mitgesendeter Geodaten ermittelbar. Sie spiegelt größtenteils die Bevölkerungsverteilung wider; Zentren sind vor allem Berlin und das Ruhrgebiet. Allerdings ist -- neben dem Twittern allgemein -- im Besonderen das Mitsenden von Geodaten (sogenanntes Geotagging) im deutschsprachigen Raum eher unpopulär.

Es gibt drei grundsätzlich verschiedene ortsbezogene Informationen, die bei einem Tweet mitgesendet werden können. Keine der Angaben ist für den Nutzer verpflichtend, sodass nicht zu allen Tweets ortsbezogene Daten verfügbar sind.
Zunächst kann der Nutzer in seinem Profil einen Standort eingeben. Er erscheint im JSON-Objekt des Tweets als Feld \textit{location} im Unterobjekt \textit{user.} In unserem Korpus ist zu x\% der Tweets diese Angabe vorhanden. Allerdings werden hier auch sehr gerne Fantasie-Orte eingetragen. Hinzu kommt die mögliche Mehrdeutigkeit und Ungenauigkeit der Angaben, weswegen sie für maschinelle Verwertung praktisch ausscheiden.
Beim Absenden eines Tweets besteht außerdem die Option \textit{Standort hinzufügen}. Dort kann ein eindeutiger, benannter Ort hinzugefügt werden. Im JSON-Objekt erscheint er als Unterobjekt \textit{place}, unter anderem mit Namen, Typ (bspw. "city" für Stadt), Land und einer \textit{bounding box}, einem Rechteck aus vier Geokoordinaten, welches den Ort einschließt.
"Tweets associated with places are not necessarily issued from that location but could also potentially be \textit{about} that location."
Diese Angabe findet sich in unserem Korpus zu x\% der Tweets.
Schließlich ist es möglich, von GPS-fähigen Geräten aus direkt den tatsächlichen Absendeort des Tweets mitzusenden. Die Koordinaten werden als Unterobjekt \textit{geo} im JSON gespeichert. x\% der Tweets in unserem Korpus tragen diese Angaben.


\subsection{General Idea}
Ziel: ungefähre regionale Einordnung eines Tweets innerhalb des deutschssprachigen Raums trotz der o. g. seltenen Geodaten und schlecht benutzbaren Herkunftsangaben %...
Idee: Sprache verrät Herkunft, also sollte aus dem reinen Tweettext die Herkunft ablesbar sein %...

Es existieren hier zwei unterschiedliche Größen. Zum einen gibt es den oder die Orte, an denen ein Twitterer aufgewachsen ist und die ihn sprachlich geprägt haben. Hauptsächlich diese Orte messen wir, wenn wir nach mundartlichen Ausdrücken und Ausdrücken der regionalen Alltagssprache suchen. Auf der anderen Seite steht der momentane, mitunter sehr kurzfristige Aufenthaltsort, von dem aus der Nutzer twittert. Auf Inhaltsebene der Tweets wird er sich eher in ortsbezogenen Begriffen (Ortsnamen, Verkehrsknotenpunkte, Lokalitäten, lokale Ereignisse und Persönlichkeiten etc.) widerspiegeln. Er ist es außerdem, den wir aus den Geodaten von Tweets erfahren. Nun sind Geodaten jedoch die einzigen Daten, die wir zur Evaluierung unserer Ergebnisse verwenden können. Während unser geodatengestützter Ansatz damit recht passend evaluiert werden kann, zielt unser Regiowort-Ansatz speziell auf die Größe \textit{sprachliche Herkunft des Twitterers} ab und wird die Evaluierung daher zwangsläufig mit einem gewissen Handicap absolvieren.

Ansatz:
\begin{itemize}
\item Einteilung des deutschsprachigen Raums in Regionen
\item Machine Learning auf Trainingsdaten aus diesen Regionen
\item Bag-of-words model (Betrachtung von Unigrammen)
\end{itemize}
%...

\subsection{Regional words approach}
This \emph{Regio-words-approach} was inspired by the funny fact that even for such ordinary things like small bakery products or other foods exist a confusing mess or words. Funny means that many of these product got names of towns different from the speakers-town. So a small sausage in Berlin is called "Wiener" whilst in Vienna it is called "Frankfurter". This shows that the use of language esp. the use of lexical items differs from throughout regions. The aim of our regional word approach was to exploit this fact and try to map different words to specific regions and to create a vector-space-models of words for each region. In doing so we should be able to compare new tweets against these models and assign it to one region. 
\subsection{Geo location approach}
As allready mentioned above german twitter users only seldom allow twitter to store geo data. But nonetheless there is a small number of tweet with these geo data. The \emph{Geo-location-approach} uses these data by exactly assign a specific region to each tweet and additionally assign all words in this tweet also to this region. This will be done for all tweets in the train corpus and so finally we created a vector-space-model again - this time based on the exact knowledge of geographical origins of all tweet writers. 
\subsection{Expectations}
We expected that it was possible to map tweets to a specific region if this tweet contains material which will be relatively prominent in this region. But we were aware about the fact that the list which was the base of our \emph{Regio-words-approach} was very vulnerable to criticism because we ourselves lay the ground for further calculations. This means that not the real use of language of twitter users were the starting point of this approach but academical investigations. The opposite is the case for the \emph{Geo-location-approach}: Here all calculations were done based on real twitter data.\\
We therefore expected our \emph{Geo-location-Approach} to show better results than the \emph{Regio-words-approach}.
\subsection{Sources, used corpora}
We created two corpora: \emph{Regional-Word-Corpus} and \emph{Geo-Data-Corpus}.
For our \emph{Reginal-Word-Corpus} we created a list of words which we thought would be good candidates to represent one or max. four regions. This list of word was inspired by the work of the \emph{Atlas zur deutschen Alltagssprache} - a project of University Salzburg and University of Liege which aim is to collect information about everdays use of different words of native german speakers depending on specific regions. This list was used to collect 12.878 tweets from 01.08.2013 to 08.08.2013 using the Twitter-API. Because we thought that some word will be unknown or very seldom we decided to switch off languge detection by \emph{LangID} and only check if a tweet containing one of our words got the Country-ID "de". We are aware of the fact that by chance some of our words could also be words in other languages but we thought that combining this word with the german Country-ID was sufficient to minimize that chance. In Addition we created a Corpus out of 12.878 tweets from the \emph{Scheffler-Corpus} (see below) for reasons of comparison.\\
Our \emph{Geo-Data-Corpus} have been extracted from the \emph{Scheffler-Corpus}.
We created balanced sets, where the amount of  Tweets are the same for all regions. Because the fewest number of Tweets (8787) came from region 6, 'Austria', we ended up with only 61509 Tweets for all regions.\\
The source for parts of the \emph{Regional-Words-Corpus} and \emph{Geo-Data-Corpus} was the \emph{Scheffler-Corpus}. This is a collection containing  tweets from 1.April 2012 to 30.April 2012 collected by using the Twitter-API and which are recognized as german tweets by \emph{LangID}.
Additionally we 
\subsection{Regions}
Die Grundüberlegungen, auf denen unsere konkrete Einteilung des deutschen Sprachraumes in Regionen basiert, gab hauptsächlich der Regiowort-Ansatz vor. Die Datenlage im Atlas der deutschen Alltagssprache zeigte uns, in welcher Größenordnung man den Sprachraum auf Basis regionaler Alltagssprache einteilen kann. Auch die konkrete Festlegung der einzelnen Regionen trafen wir beim Sichten der dortigen Daten. Dennoch trug hier auch Twitter als unser Anwendungsbereich ein entscheidendes Kriterium bei: Es sollten Regionen entstehen, in denen jeweils mit einem genügend großen Aufkommen von Tweets zu rechnen war. So zeigte sich in den Daten des Atlas der deutschen Alltagssprache auch eine kleinere Region im Gebiet Saarland/Luxemburg mit charakteristischen Eigenheiten, die wir allerdings wegen zu geringer erwartbarer Menge von Tweets nicht übernahmen.

\end{document}
